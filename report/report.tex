\documentclass[sigconf, nonacm]{acmart}
\settopmatter{printfolios=true}
\settopmatter{printacmref=false}
%% PACKAGES
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{xcolor}

%% COLORS
\definecolor{darkgreen}{rgb}{0,0.8,0}

%% TITLES
\title{Molecule-Text contrastive learning for retrieval systems of chemical compounds}

%% AUTHORS
\author{Balthazar Neveu}
\affiliation{%
  \institution{ENS Paris-Saclay}
  \city{Saclay}
  \country{France}
}
\email{balthazar.neveu@ens-paris-saclay.fr}



%% MAIN DOCUMENT
\begin{document}

  %% KEYWORDS
  \keywords{contrastive learning, graph neural networks, large language models}

  %% Teaser figure
  \begin{teaserfigure}
    \includegraphics[width=1.\textwidth]{figures/mol_text_overview.PNG}
    \centering
    \caption{On the left side, text descriptions are transformed into sequences of tokens of various lengths. Tokenized sequences are then embedded into a vector space using a language model encoder. Each description $T_{i}$ is transformed into a vector $t_{i}$ (\color{lime} text descriptions embeddings\color{black}). In the middle, molecules $M_{i}$ will be transformed into vector descriptors $m_{i}$ (\color{purple}{molecule embeddings}\color{black}). Each atom is first transformed into a vector using its neighboring atoms. The graph structure is kept as an undirected graphs to model the bounds between atoms. A graph neural network is then used to embed these graphs into a vector space. The molecule and text descriptions embeddings are then compared. At inference time, a text sentence $T$ will be embedded into a vector $t$ which is comparable with relevant matching molecules. The closest molecule embeddings can be proposed to the chemist. Contrastive learning is used to train such a model: supervision comes from the knowlege of the pairing between the molecule and its text description. This is shown on the right side where we compute the similarity between the molecule embedding $m_{i}$ and text embeddings $t_{i}$. The idea behing contrastive learning relies on maximizing similarity between molecules and text embeddings for the correct pair and minimized for the wrong pairs.
    }
    \label{fig:original_pipeline}
  \end{teaserfigure}
  %% TITLE
  \maketitle


  %% CONTENT
  \input{content/1_introduction.tex}
  \input{content/2_context.tex}
  \input{content/3_our_work.tex}
  \input{content/4_discussion.tex}
  \input{content/5_conclusion.tex}

  \newpage

  %% APPENDIX
  \appendix
  \input{content/appendix.tex}
  
  \newpage
  %% BIBLIOGRAPHY
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{references}

\end{document}
